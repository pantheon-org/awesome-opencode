/**
 * Security alerting module
 *
 * Creates alerts when suspicious activity is detected:
 * - GitHub issues for security team
 * - Comments on source issues/PRs
 * - Optional webhook notifications
 *
 * Features:
 * - Automated security issue creation
 * - User-facing warnings
 * - Pattern-specific alert details
 * - Actionable next steps for security team
 */

import type { SecurityAlertParams, InjectionPattern } from './types';
import { loadSecurityConfig } from './config';

/**
 * Format injection patterns for display
 * Converts pattern enum to human-readable text
 *
 * @param patterns - Array of injection patterns
 * @returns Formatted markdown list
 */
function formatPatterns(patterns: InjectionPattern[]): string {
  const patternDescriptions: Record<InjectionPattern, string> = {
    'role-switching': 'Role Switching (e.g., "ignore previous instructions")',
    'instruction-override': 'Instruction Override (e.g., "your new task is")',
    'delimiter-injection': 'Delimiter Injection (attempting to close system prompts)',
    'context-confusion': 'Context Confusion (manipulating conversation structure)',
    'encoded-payload': 'Encoded Payload (base64 or URL-encoded injection)',
    'url-injection': 'URL Injection (malicious URLs or path traversal)',
    unknown: 'Unknown Pattern (flagged by security system)',
  };

  return patterns.map((pattern) => `- **${patternDescriptions[pattern] || pattern}**`).join('\n');
}

/**
 * Create a security alert issue
 * Posts a new GitHub issue with security alert details
 *
 * @param params - Alert parameters including GitHub client and context
 *
 * @example
 * ```typescript
 * await createSecurityAlert({
 *   github,
 *   context,
 *   user: 'suspicious_user',
 *   attempts: 5,
 *   patterns: ['role-switching', 'instruction-override'],
 *   issueNumber: 123
 * });
 * ```
 */
export async function createSecurityAlert(params: SecurityAlertParams): Promise<void> {
  const { github, context, user, attempts, patterns, issueNumber } = params;
  const config = loadSecurityConfig();

  // Check if alerting is enabled
  if (!config.alerting.enabled || !config.alerting.createIssue) {
    return;
  }

  const issueRef = issueNumber ? `#${issueNumber}` : 'multiple submissions';
  const timestamp = new Date().toISOString();

  const body = `## üö® Security Alert: Prompt Injection Attempts Detected

**User:** @${user}  
**Total Attempts:** ${attempts}  
**Source:** ${issueRef}  
**Detected At:** ${timestamp}

### Patterns Detected

${formatPatterns(patterns)}

### Action Taken

The user has been rate-limited and their submission(s) have been flagged for review pending investigation.

### Next Steps

1. **Review User Activity**
   - Check user's recent issues and PRs for malicious intent
   - Look for patterns across multiple submissions
   - Review user's GitHub profile and contribution history

2. **Analyze Patterns**
   - Determine if detections are legitimate threats or false positives
   - Check if patterns are part of legitimate tool descriptions
   - Consult with security team if unclear

3. **Take Action**
   - If confirmed malicious: Add user to block list, close related issues
   - If false positive: Reset rate limit, approve submissions, update detection rules
   - Document decision in comments below

4. **Update Detection Rules** (if false positive)
   - Review pattern matching in \`src/security/sanitize-ai-input.ts\`
   - Add exceptions for legitimate use cases
   - Test changes thoroughly before deploying

### Resources

- [Security Documentation](../SECURITY.md)
- [Injection Detection Code](../src/security/sanitize-ai-input.ts)
- [Rate Limiting Config](../config/security.json)

---

**Note:** This is an automated alert generated by the security monitoring system.`;

  try {
    await github.rest.issues.create({
      owner: context.repo.owner,
      repo: context.repo.repo,
      title: `[Security] Prompt injection attempts by @${user}`,
      body,
      labels: ['security', 'prompt-injection', 'needs-review'],
    });
  } catch (error) {
    console.error('Failed to create security alert:', error);
  }
}

/**
 * Post a warning comment on the source issue/PR
 * Informs the user that their submission was flagged
 *
 * @param params - Alert parameters including GitHub client and context
 *
 * @example
 * ```typescript
 * await postSecurityWarning({
 *   github,
 *   context,
 *   user: 'username',
 *   attempts: 3,
 *   patterns: ['role-switching'],
 *   issueNumber: 123
 * });
 * ```
 */
export async function postSecurityWarning(params: SecurityAlertParams): Promise<void> {
  const { github, context, user, attempts, patterns, issueNumber } = params;
  const config = loadSecurityConfig();

  // Check if commenting is enabled
  if (!config.alerting.enabled || !config.alerting.commentOnSource || !issueNumber) {
    return;
  }

  const body = `## ‚ö†Ô∏è Security Warning

@${user}, your submission has been flagged by our security monitoring system.

**Reason:** The content appears to contain patterns commonly associated with prompt injection attacks.

**Patterns Detected:**
${formatPatterns(patterns)}

### What This Means

Our automated security system detected potential prompt injection patterns in your submission. This could be:

1. **False Positive:** Your tool legitimately discusses AI security, prompt engineering, or related topics
2. **Actual Issue:** Your submission contains malicious content

### Next Steps

If this is a **false positive** and your submission is legitimate:
- Please clarify in a comment that this is a legitimate AI/ML security tool
- Provide additional context about why these patterns appear
- A maintainer will review and approve your submission

If you're unsure why this was flagged, please review our [Security Guidelines](../SECURITY.md).

### Note for Maintainers

This submission has been rate-limited (${attempts} attempts detected). Please review before proceeding.

---

*This is an automated message from the security monitoring system.*`;

  try {
    await github.rest.issues.createComment({
      owner: context.repo.owner,
      repo: context.repo.repo,
      issue_number: issueNumber,
      body,
    });
  } catch (error) {
    console.error('Failed to post security warning:', error);
  }
}

/**
 * Add security labels to an issue/PR
 * Marks submissions as security-related for tracking
 *
 * @param params - Alert parameters
 */
export async function addSecurityLabels(params: SecurityAlertParams): Promise<void> {
  const { github, context, issueNumber } = params;

  if (!issueNumber) {
    return;
  }

  try {
    await github.rest.issues.addLabels({
      owner: context.repo.owner,
      repo: context.repo.repo,
      issue_number: issueNumber,
      labels: ['security-flagged', 'needs-review'],
    });
  } catch (error) {
    console.error('Failed to add security labels:', error);
  }
}

/**
 * Close and lock an issue/PR that exceeded rate limits
 * Used for confirmed malicious activity
 *
 * @param params - Alert parameters
 */
export async function blockIssue(params: SecurityAlertParams): Promise<void> {
  const { github, context, issueNumber } = params;

  if (!issueNumber) {
    return;
  }

  try {
    // Close the issue
    await github.rest.issues.update({
      owner: context.repo.owner,
      repo: context.repo.repo,
      issue_number: issueNumber,
      state: 'closed',
      labels: ['security-blocked', 'prompt-injection'],
    });

    // Lock the issue to prevent further comments
    await github.rest.issues.lock({
      owner: context.repo.owner,
      repo: context.repo.repo,
      issue_number: issueNumber,
      lock_reason: 'spam',
    });

    // Post a final comment
    await github.rest.issues.createComment({
      owner: context.repo.owner,
      repo: context.repo.repo,
      issue_number: issueNumber,
      body: `This submission has been closed due to repeated security violations (prompt injection attempts).

If you believe this is an error, please contact the maintainers directly.

For more information, see our [Security Policy](../SECURITY.md).`,
    });
  } catch (error) {
    console.error('Failed to block issue:', error);
  }
}

/**
 * Send webhook notification for external alerting
 * Useful for Slack, Discord, or other monitoring systems
 *
 * @param params - Alert parameters
 */
export async function sendWebhookAlert(params: SecurityAlertParams): Promise<void> {
  const { user, attempts, patterns } = params;
  const config = loadSecurityConfig();

  // Check if webhook is configured
  if (!config.alerting.enabled || !config.alerting.webhookUrl) {
    return;
  }

  const payload = {
    text: `üö® Security Alert: Prompt injection attempts detected`,
    blocks: [
      {
        type: 'header',
        text: {
          type: 'plain_text',
          text: 'üö® Security Alert: Prompt Injection Detected',
        },
      },
      {
        type: 'section',
        fields: [
          {
            type: 'mrkdwn',
            text: `*User:*\n@${user}`,
          },
          {
            type: 'mrkdwn',
            text: `*Attempts:*\n${attempts}`,
          },
        ],
      },
      {
        type: 'section',
        text: {
          type: 'mrkdwn',
          text: `*Patterns:*\n${patterns.join(', ')}`,
        },
      },
    ],
  };

  try {
    const response = await fetch(config.alerting.webhookUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(payload),
    });

    if (!response.ok) {
      console.error('Webhook alert failed:', response.statusText);
    }
  } catch (error) {
    console.error('Failed to send webhook alert:', error);
  }
}

/**
 * Handle a security incident with full alerting
 * Combines all alerting mechanisms based on severity
 *
 * @param params - Alert parameters
 * @param severity - Severity level ('low' | 'medium' | 'high')
 *
 * @example
 * ```typescript
 * // Low severity: Just warn the user
 * await handleSecurityIncident(params, 'low');
 *
 * // High severity: Create alert, warn user, block issue
 * await handleSecurityIncident(params, 'high');
 * ```
 */
export async function handleSecurityIncident(
  params: SecurityAlertParams,
  severity: 'low' | 'medium' | 'high' = 'medium',
): Promise<void> {
  // Always add security labels
  await addSecurityLabels(params);

  if (severity === 'low') {
    // Low severity: Just warn the user
    await postSecurityWarning(params);
  } else if (severity === 'medium') {
    // Medium severity: Warn user and create alert
    await postSecurityWarning(params);
    await createSecurityAlert(params);
    await sendWebhookAlert(params);
  } else if (severity === 'high') {
    // High severity: Full response including blocking
    await createSecurityAlert(params);
    await postSecurityWarning(params);
    await blockIssue(params);
    await sendWebhookAlert(params);
  }
}
